\documentclass[twoside,11pt]{article}

\usepackage{paper}
\usepackage{tikz}     
\usepackage{xcolor}
\usepackage{amsmath}
\usetikzlibrary{calc}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\definecolor{darkorange}{RGB}{204,85,0}

\ShortHeadings{
%% short title here
Toward a Manifold RD Estimator
}{
% last name
Posmik}
\firstpageno{1}

\begin{document}

\title{Bayesian Topological Weights in Causal Inference -- \\ Toward a Manifold RD Estimator \\
\vspace{.1in}
PHP2530			
}

\author{ Daniel C. Posmik }

\maketitle
\date{4 }

\section{Introduction} \label{sc:intro}

\textcolor{red}{There's a parellel to random effects; Call them Bayesian topo weights; Algorithm: while $i < N$, 1) Draw $\theta_i$ from posterior, 2) get $kappa_i$ at that $\theta_i$, 3) Calculate weight}

Manifold learning is dimensionality reduction technique that has proven useful in settings where data are high-dimensional and non-linear. Often, manifold learning algorithms are used when the topological structure of the data are to be preserved in a statistical learning task\footnote{For an introduction, see \citet{Meila2023}}. Within the manifold learning framework, data are assumed to live on a lower dimensional manifold and are corrupted by high-dimensional noise. We say that $D$-dimensional data can be embedded in $d$-dimensional manifold where $d \leq D$ but generally $d \ll D$ .  

\begin{figure}[h!]
  \begin{center}
    \begin{tikzpicture}[scale=0.75]
      % Define styles
      \tikzset{
        manifold/.style={blue, thick, fill=blue!10},
        noise/.style={gray, thick},
        observed/.style={blue, thick, fill=blue!10, opacity=0.7},
        arrow/.style={->, >=stealth, thick},
        label/.style={font=\normalfont\normalsize}
      }
      % Low-dimensional manifold (left)
      \begin{scope}[xshift=-5cm]
        \draw[manifold] plot[smooth, tension=0.8] coordinates {(-1.5,0) (-0.5,1) (1,1.2) (1.5,0) (1,-1) (-0.5,-1) (-1.5,0)};
        
        % Draw some points on the manifold
        \node[circle, draw, inner sep=1pt, fill=blue!50] at (-0.7,0.3) {$f$};
        \node[circle, draw, inner sep=1pt, fill=blue!50] at (0.8,0.5) {$f$};
        \node[circle, draw, inner sep=1pt, fill=blue!50] at (0.2,-0.6) {$f$};
      \end{scope}
      % High-dimensional noise (center)
      \begin{scope}
        % Draw noise as radial lines from center point
        \fill (0,0) circle (0.1);
        \foreach \i in {0,10,...,350} {
          \draw[noise] (0,0) -- (\i:1.2+0.3*rnd);
        }
      \end{scope}
      % Observed data (right)
      \begin{scope}[xshift=5cm]
        % First draw the noise pattern
        \fill (0,0) circle (0.1);
        \foreach \i in {0,10,...,350} {
          \draw[noise] (0,0) -- (\i:1.2+0.3*rnd);
        }
        
        % Then overlay the manifold with transparency
        \draw[observed] plot[smooth, tension=0.8] coordinates {(-1.5,0) (-0.5,1) (1,1.2) (1.5,0) (1,-1) (-0.5,-1) (-1.5,0)};
        
        % Draw some points on the manifold
        \node[circle, draw, inner sep=1pt, fill=blue!50, opacity=0.7] at (-0.7,0.3) {$f$};
        \node[circle, draw, inner sep=1pt, fill=blue!50, opacity=0.7] at (0.8,0.5) {$f$};
        \node[circle, draw, inner sep=1pt, fill=blue!50, opacity=0.7] at (0.2,-0.6) {$f$};
      \end{scope}
      % Draw the operation symbols
      \node at (-2.5,0) {$+$};
      \draw[arrow] (2,0.5) -- (3,0.5);
      \draw[arrow] (3,-0.5) -- (2,-0.5);
      % Draw the curved arrows for parametrization and embedding
      \draw[arrow, green!50!black, thick] (-4,2.5) to[bend left=30] node[above, font=\normalfont\normalsize] {Parameterization} (4,2.5);
      \draw[arrow, orange, thick] (4,-2.5) to[bend left=30] node[below, font=\normalfont\normalsize] {Embedding} (-4,-2.5);
      % Labels with consistent position and size
      \node[label, text width=3cm, align=center] at (-5,-2) {Low-dimensional manifold};
      \node[label, text width=3cm, align=center] at (0,-2) {High-dimensional noise};
      \node[label, text width=3cm, align=center] at (5,-2) {Observed data};
    \end{tikzpicture}
  \end{center}
  \caption{Manifold Learning: Parameterization vs. Embedding}\label{fig:manifolds}
\end{figure}

Within the principal manifolds framework \citep{Meng2021} -- a replicable and flexible framework for manifold learning -- the process of fitting a manifold to our data contains multiple steps. The key idea of the fitting step is that we fit a $d$-dimensional manifold to our $D$-dimensional by minimizing the sum of squares between our data and the proposed manifold. An important extension to linear dimensionality reduction, i.e. the principal components algorithm (PCA), is that we allow our proposed manifold to preserve underlying topological structure of our data. In a way, manifold learning reduces the dimensionality of data with an explicit focus on the topology of it. We note that -- although certainly intuitive -- this topological structure is not only limited to spatial abstraction, but may be extended to arbitrary dimensions of interest. This framework was pioneered as an extension to the PCA algorithm with curves (\citet{HastieStuetzle1989}, \citet{Tibshirani1992}) and has since found a myriad of applications in higher-dimensional settings. 

We now propose a method for incorporating prior distributional information into the principal manifolds framework. 

\section{Background}\label{sc:background}

Consider a setting where we have fit a manifold $\mathcal{M}_d$ to our $D$-dimensional data by means of minimizing the orthogonal distance between the data and the manifold. We consider this manifold fixed and will not touch on the fitting procedure itself. Given $\mathcal{M}_d$, for each data point, i.e. the row vector $[x_{11} \cdots x_{1D}]^T$, we can now define the point on $\mathcal{M}_d$, say $f\left(\left[x_{11} \cdots x_{1D}\right]^T\right)$. This point minimizes the distance between $x_i$ and $f(x_i)$. We want to stress again that this procedure does not mean we are fitting the manifold to the data, we are simply retrieving the distance-minimizing projection point. We write 

$$
\text{arg~min}_{f \in \mathcal{F}} \|x^* - f(x^*)\|_2
$$

where we consider each projection function $f$ to be a member of an arbitrary Sobolev space $\mathcal{F}$. We define the distance metric as the $L^2$ distance. 

If there exists only one projection point $f(x_i)$ for every $x_i$, every $f \in \mathcal{F}$ is a one-to-one and onto ("bijective") mapping. We find it interesting to highlight that the projection functions in the PCA algorithm are inherently bijective, and for inferential purposes, this is a property that is often taken for granted\footnote{This is because a principal axes is a straight line, i.e. neither convex or concave. Although a point's distance to its projection may be co-minimal across $\leq 2$ dimensions, it only has one $f(x_i)$ in one principal axis.}. In a manifold learning framework, this is no longer the case. Albeit highly interesting, due to the limited scope of this paper, we shall treat this scenario as an edge case, reserving rigorous treatment for the blissful times that follow the author's qualifying exam. 

Now, given the data $[\{x_i\}_{i=1}^n, \{f(x_i)\}_{i=1}^n]$, we can reparameterize our space into polar coordinates to obtain a vector representation of the collection $f \in \mathcal{F}$. Converting a Cartesian parameterization in space with $D$ dimensions into polar coordinates yields the $d$-dimensional vector $[r_i^*; \theta_{i, 1}, \cdots, \theta_{i, D-1}]$, i.e. one radius $r_i^*$ and a set of $D-1$ angles suffice to characterize each point $x_i$'s location in space. 

Recognize that the parameter $r^*$ is not random. This is because it is simply the result from our previous projection distance-minimizing procedure. Usually, polar parameterizations assume that all angles and radii are centered at the origin. Luckily, simple vector addition and subtraction readily generalizes our parameterizations in space. For instance, to obtain the vector from the point $x_i$ and $f(x_i)$, we simply subtract $f(x_i) - x_i$. It is important that the issue of defining the origin is explicitly clarified when dealing with directional information. For simplicity, we will henceforth consider data centered at the origin.

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.75\textwidth]{../fig/data-with-noise.png}
  \end{center}
  \caption{Noisy data projected on $\mathcal{M}_3$; the unit sphere in $\mathbb{R}^3$}\label{fig:data-with-noise}
\end{figure}


\section{Directional Priors}\label{sc:directional-priors}

In contrast to the radii, the $\{D-1\}$-dimensional vector of angles, $\boldsymbol{\theta_i} := [\theta_{i, 1}, \cdots, \theta_{i, D-1}]^T$ is random. In simple terms, $\{r_i, \boldsymbol{\theta}_i\}$ is what parameterizes the realized sample $\boldsymbol{X_i} = x_i$ in space. If we draw multiple samples, the random sampling variation in $\mathbf{\theta}_i$ is what captures the randomness. In biomedical applications, such as cancer medicine, we may have reliable prior information (i.e. from previous trials or expert knowledge) on directional trends of malignant growths. Within a Bayesian framework, using our data to update these prior directional information offers a principled, probabilistic solution to complex inference problems in settings where directionality is a key piece of information.   

Before we formulate our approach formally, we briefly introduce directional statistics. When we reparameterized our data from Cartesian coordinates into polar coordinates, we did not address the underlying probabilistic transformations. For example, when encoding uncertainty in a spherical setting, it may be naive to parameterize a normal density with support on $\mathbb{R}^1$ since angles are defined in the interval $[0, 2\pi]$. Directional statistics offer well-defined spherical reparameterizations of distributions, such as the normal. Instead of defining a mean vector in $\mathbb{R}^1$, we can define a mean directional vector contained in $[0, 2\pi]$. To account for the infinite support, the wrapped normal is defined on a support of $k\text{mod}(2\pi)$. Henceforth, unless stated otherwise, we will deal with the spherical parameterizations of densities to account for the spherical parameterization of our problem. 

\subsection{Motivating Example: 3D Sphere}

We begin our discussion with an example of a 3D sphere. As discussed, this means that we have some simulated noisy data $\{x_i\}_{i=1}^n$ in 3D space. The noisy data is projected on the manifold of choice, i.e. the unit sphere. This yields the data $\{f(x_i)\}_{i=1}^n$. These data are parameterized in a polar coordinate system with $\{r_i, \mathbf{\theta}_i\}$, where the (non-random) radius $\vec{r_i}$ is the vector from the origin to $f(x_i)$. Since we are in 3D space, we have two angles. One angle, $\{\mathbf{\theta}\}_{i=1}^n$, which parameterizes the angle from the origin on the XY-plane. The angle $\{\mathbf{\phi}\}_{i=1}^n$ is the angle between the XY-plane and the point $f(x_i)$. 

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.75\textwidth]{../fig/projections-from-origin.png}
  \end{center}
  \caption{Polar parameterization with $\{r_i, \theta_i, \phi_i\}$ of projected data}\label{fig:projections-from-origin}
\end{figure}

\subsection{Formulating Directional Priors}\label{sc:dir-priors}

Formulating a directional prior requires careful consideration of reference points. All angles must be measured from the same reference point—for simplicity, we choose the origin. A directional prior with mean direction $\mu \in [0, \pi]$ allows us to incorporate information about where projected points should concentrate on the manifold. We only posit a prior on the random set $\{\theta_i\}_{i=1}^n$, expressing prior distributional beliefs on point concentrations in the XY-plane. For this example, we consider angles $\{\phi_i\}_{i=1}^n$ fixed, making our example univariate. Extension to the multivariate case is straightforward using multivariate analogs of our likelihood and prior distributions.

Our posterior distribution on the random vector $\theta$ can be expressed as:

$$
\begin{aligned}
  f_{\theta | \mu} (\theta | \mu) 
  &\propto \left\{ \prod_{i=1}^n f_{\theta_i | \mu}(\theta_i | \mu) \right\} \cdot f_{\mu}(\mu) \\ 
  &\propto \mathcal{L}(\theta | \mu) \cdot f_{\mu}(\mu) 
\end{aligned}
$$

where $\mu$ is our mean vector with prior $\mu \sim f_{\mu}(\mu)$. For our likelihood, we initially consider the wrapped normal distribution with density:

$$
f(\theta|\mu,\sigma^2) = 
\frac{1}{2\pi\sigma^2} \sum_{k=-\infty}^{\infty} \exp\left(-\frac{(\theta-\mu+2\pi k)^2}{2\sigma^2}\right)
$$

However, this infinite series presents computational challenges. Instead, we use the von Mises distribution, which provides a more tractable circular analog to the normal distribution:

$$
f(\theta|\mu,\kappa) = \frac{e^{\kappa \cos(\theta-\mu)}}{2\pi I_0(\kappa)}
$$

where $\theta$ is the angle, $\mu$ is the mean direction, $\kappa \geq 0$ is the concentration parameter (analogous to precision, or inverse variance), and $I_0(\kappa)$ is the modified Bessel function of first kind of order 0. As $\kappa$ increases, the distribution concentrates more tightly around $\mu$. When $\kappa = 0$, the distribution becomes uniform on the circle. The von Mises distribution scales naturally for hyperspheres in $\mathbb{R}^p$, making it ideal for spherical settings in manifold learning applications.

\subsection{Variance Encodes Curvature}\label{sc:curvature}

For our concentration parameter $\kappa$, naive selection (e.g., a constant) would be possible but would negate the benefits of manifold learning as a topology-preserving dimensionality reduction technique. Instead, we propose a principled approach that encodes local topological structure into the variance parameter. Intuitively, in regions with little topological variation, we might update our projection more liberally. Conversely, if a small change in projection angle moves our data point into a topologically distinct region, we would be more conservative with updates.

In spherically parameterized manifolds, the inverse absolute Gaussian curvature provides a natural variance encoding. This approach has an elegant connection to the score test in likelihood-based inference, which uses the slope at restricted MLE estimates as a similarity measure. Thus, our solution has both topological intuition and established statistical parallels in traditional inference methods.

The Gaussian curvature of a surface can be defined as the product of the principal curvatures:

$$K = k_1 \cdot k_2$$

where $k_1$ and $k_2$ are the principal curvatures at a point on the surface. For a sphere of radius $R$, due to perfect symmetry, both principal curvatures equal $\frac{1}{R}$, giving a Gaussian curvature of $K = \frac{1}{R^2}$. For a unit sphere, the Gaussian curvature is $K = 1$, an intrinsic property related to the total curvature integrated over the entire sphere equaling $4\pi$ by the Gauss-Bonnet theorem. This curvature-based approach provides a meaningful way to weight observations based on their location in the manifold's topological structure. Areas with high curvature (significant bending of the manifold) will contribute less to updating our beliefs than areas with lower curvature (flatter regions), which aligns with our geometric intuition about confidence in projections.

\subsection{Conjugacy Results}\label{sc:conjugacy}

A key advantage of the von Mises distribution is conjugacy, which allows for straightforward Bayesian updates of our directional priors. Following the work of \citet{mardia1976}, a von Mises prior with mean direction $\mu$ and concentration parameter $\kappa$ is conjugate to the likelihood. After observing angles $\theta_1,\ldots,\theta_n$, the posterior distribution takes the form:

$$
f(\mu_i, \mu^* | \{\theta_i\}_{i=1}^n) \propto \exp(\kappa \cdot \sum_{i=1}^n \cos(\theta_i - \mu_i) +\kappa^* \cdot \sum_{i=1}^n \cos(\mu_i - \mu^*))
$$

where angles $\{\theta_i\}_{i=1}^n \sim \mathcal{VM}(\mu_i, \kappa)$ and the prior is $\mu_i \sim \mathcal{VM}(\mu^*, \kappa^*)$. In our case, the data were generated with $\mu_i = \mu = \text{circular}(0)$.

The posterior form reveals a shrinkage estimator structure. The component involving observed angles $\{\theta_i\}_{i=1}^n$ is weighted by $\kappa$, which in our framework represents the curvature at the projected point. The difference between the data mean and prior mean is weighted by the prior concentration parameter $\kappa^*$. This structure extends shrinkage estimation to our topological interpretation: when the data concentration parameter $\kappa$ is low, more weight is attributed to our prior information.

For our 3D spherical example, given a prior $\boldsymbol{\theta} \sim \mathcal{VM}(\theta_0, \tau_0 = \frac{1}{\kappa_0})$, the posterior mean is:

$$
\underbrace{\frac{\tau_0}{\tau_0 + \hat{\tau}} \cdot \theta_0}_{\text{Weighted Prior Vector}} + \underbrace{\frac{\hat{\tau}}{\tau_0 + \hat{\tau}} \cdot \bar{\theta}}_{\text{Weighted Data Vector}}
$$

with posterior variance $(\tau_0 + \hat{\tau})^{-1}$, where $\bar{\theta} := \frac{1}{n}\sum_{i=1}^n \theta_i$ and $\hat{\tau} = \frac{1}{\hat{\kappa}}$ with $\hat{\kappa}$ being the observed Gaussian curvature at the manifold point. The shrinkage toward the prior mean is immediately evident in this expression.

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=1\textwidth]{../fig/posterior-roseplot.png}
  \end{center}
  \caption{Posterior distribution under prior $\theta_0$: Left $\theta_0 = \pi/2$; Right: $\theta_0 = \pi$}\label{fig:posteriors}
\end{figure}

Figure \ref{fig:posteriors} demonstrates that the posterior mean effectively balances the prior and data means. This occurs because our manifold is a sphere with equivalent curvature at every point. For manifolds with heterogeneous curvature, we would observe less reprojection with respect to the prior mean in regions where curvature is more extreme, thus preserving the topological characteristics of the data.

The conjugacy relationship for von Mises distributions has an elegant geometric interpretation. For a von Mises likelihood $f(x|\mu) \propto \exp(\kappa \cos(x-\mu))$ and a von Mises prior on the mean direction $\pi(\mu) \propto \exp(\kappa_0 \cos(\mu-\mu_0))$, the posterior distribution is also von Mises: $\pi(\mu|x) \propto \exp(\kappa_n \cos(\mu-\mu_n))$. The posterior parameters can be expressed in terms of resultant vectors:

$$\kappa_n\cos(\mu_n) = \kappa_0\cos(\mu_0) + \kappa\cos(x)$$
$$\kappa_n\sin(\mu_n) = \kappa_0\sin(\mu_0) + \kappa\sin(x)$$

These equations can be rewritten in vector form as:

$$\kappa_n = \sqrt{(\kappa_0\cos(\mu_0) + \kappa\cos(x))^2 + (\kappa_0\sin(\mu_0) + \kappa\sin(x))^2}$$

and 

$$\mu_n = \text{atan2}(\kappa_0\sin(\mu_0) + \kappa\sin(x), \kappa_0\cos(\mu_0) + \kappa\cos(x))$$

This representation yields a powerful geometric interpretation: the posterior mean direction and concentration are determined by the vector sum of the prior and likelihood information vectors in $\mathbb{R}^2$. Each vector has direction given by unit vectors pointing at angles $\mu_0$ and $x$ respectively, with magnitude determined by concentration parameters $\kappa_0$ and $\kappa$. The resultant vector's length determines the posterior concentration parameter $\kappa_n$, while its direction determines the posterior mean direction $\mu_n$. 

This insight demonstrates that when prior and likelihood point in similar directions, the posterior becomes more concentrated (higher $\kappa_n$), reflecting increased certainty. Conversely, conflicting directions lead to a less concentrated posterior (lower $\kappa_n$), reflecting uncertainty.

The posterior mean naturally represents a weighted compromise between prior and data, with weights proportional to their respective concentration parameters. For multiple observations, the vectors from each observation simply add to the prior vector, providing an efficient sequential updating mechanism. This vector-based interpretation elegantly connects Bayesian inference on the circle with vector addition in the plane, making it particularly suitable for our manifold learning framework where directional data naturally arises from projections onto curved surfaces.

\section{A Causal Extension -- Toward a Manifold RD Estimator}\label{sc:causal}

Our goal is to propose an estimator that exploits topological information from pre- and post-treatment manifolds. Before we begin, recall the definition of a regression discontinuity (RD) estimator when treatment is administered at time $t^*$. 

\[
  \tau(t^*) = \lim_{\delta \rightarrow 0} \mathbb{E}[\theta | Z = 1, t = t^* + \delta] - \lim_{\delta \rightarrow 0} \mathbb{E}[\theta | Z = 0, t = t^* - \delta] 
\]

As it relates to the principal manifolds framework, after treatment happens at $t=t^*$, we can fit the post-treatment manifold $\mathcal{M}_d^{t \geq t^*}$. In the spirit of RD-based causal estimation, we want to emphasize that $\mathcal{M}_d^{t \geq t^*}$ is \textit{not} fit iteratively for each post-treatment unit, i.e. we do not obtain the post-treatment unit by combining the post-treatment units with pre-treatment units. Instead, we fit two manifolds with disjoint pre- and post-treatment data.\footnote{That being said, it may be interesting to let the choice of manifold (e.g., the shape, highest order, etc.) from the pre-treatment case inform the post-treatment case, effectively borrowing prior topological information on a more global level. This is a really interesting direction in the Bayesian paradigm and touches on the notion of persistent homology. Although this discussion goes beyond the scope of this paper, it highlights how nuanced various levels/ sources of topological information can be.} Namely, we observe the outcomes of $m$ additional units in the immediate post-treatment period, and fit $\mathcal{M}_d^{t \geq t^*}$ with data $\{\theta_{t}\}_{i=n+1}^{m}|_{t \geq t^*}$. The mean projection angle of $\mathcal{M}_d^{t \geq t^*}$, $\bar{\theta}^{t \geq t^*}$, is the post-treatment mean projection angle. It reflects the directionality under treatment. The following figure presents this admittedly complex idea visually. 

\begin{figure}[h!]
  \begin{center}
\begin{tikzpicture}[scale=0.9]
    % Define colors
    \definecolor{darkgrey}{RGB}{80,80,80}
    \definecolor{darkorange}{RGB}{204,85,0}
    
    % Time axis
    \draw[->, thick] (-0.5,0) -- (10.5,0) node[right] {$t$};
   
    % Pre-treatment and post-treatment labels
    \node[align=center, text width=4cm] at (2.5,-0.5) {\text{Pre-treatment}};
    \node[align=center, text width=4cm] at (7.5,-0.5) {\text{Post-treatment}}; 
    
    % Treatment cutoff vertical line
    \draw[dashed, thick] (5,0) -- (5,7);
    \node at (5,-0.5) {$t^*$};
    
    % Pre-treatment manifold as a regular ellipse (dark grey)
    \draw[thick, darkgrey] (2.5,3) ellipse (1.5 and 2);
    \node[darkgrey] at (2.5,5.7) {$\mathcal{M}_d^{t < t^*}$};
    
    % Pre-treatment projection angle (dark grey)
    \draw[->, thick, darkgrey] (2.5,3) -- (3.5,4.4);
    \node[darkgrey] at (4.2, 4.5) {$\bar{\theta}^{t < t^*}$};
    
    % Post-treatment: ghost of pre-treatment manifold (dashed)
    \draw[thick, dashed, darkgrey] (7.5,3) ellipse (1.5 and 2);
    
    % Post-treatment manifold as a pointier ellipse, rotated 12 degrees (darkorange)
    \begin{scope}[shift={(7.5,3)}, rotate=12]
        \draw[thick, darkorange] (0,0) ellipse (1.2 and 2.2);
    \end{scope}
    \node[darkorange] at (7.5,5.7) {$\mathcal{M}_d^{t \geq t^*}$};
    
    % Two projection angles in post-treatment period
    \draw[->, thick, darkorange] (7.5,3) -- (7.8,4.85);  % Updated angle (darkorange)
    \node[darkorange] at (8.5, 5.2) {$\bar{\theta}^{t \geq t^*}$};
    \draw[->, thick, darkgrey] (7.5,3) -- (8.6,4.4);  % Original angle direction (dashed, dark grey)
     \node[darkgrey] at (9.4, 4.5) {$\bar{\theta}^{t < t^*}$};
    
    % Data points on pre-treatment manifold (dark grey)
    \foreach \x/\y in {1.5/3.5, 2/2, 3/2, 3.5/3.5, 2.5/4.5, 2.5/1.5} {
        \fill[darkgrey] (\x,\y) circle (2pt);
    }

    % Data points on post-treatment manifold (dark grey)
    \foreach \x/\y in {6.8/3.8, 7/2, 8/2.2, 8.3/3.7, 7.5/4.8, 7.3/1.3, 7/5} {
        \fill[darkorange] (\x,\y) circle (2pt);
    }
    
    % Legend
    \begin{scope}[shift={(0,-1.5)}]
        % Rectangle around the legend - made even wider
        \draw[black] (0,0) rectangle (11,-1.5);
        
        % Legend title
        %\node[align=center] at (5.5,-.3) {\textbf{Legend}};
        
        % Pre-treatment elements (darkgrey)
        \draw[thick, darkgrey] (1.5,-0.7) -- (2.5,-0.7);
        \node[darkgrey, align=left, text width=7.5cm] at (6.8,-0.7) {Pre-treatment manifold and projection};
        
        % Post-treatment elements (darkorange)
        \draw[thick, darkorange] (1.5,-1.2) -- (2.5,-1.2);
        \node[darkorange, align=left, text width=7.5cm] at (6.8,-1.2) {Post-treatment manifold and projection};
    \end{scope} 
  \end{tikzpicture}
  \end{center}
  \caption{The Manifold RD Estimator}\label{fig:manifold-rd}
\end{figure}

\subsection{The Manifold RD Estimator}

It is common to estimate the causal effect, write $\hat{\tau}(t^*)$, using a weighting approach \textcolor{red}{cite these bad bois}. Units that are located closer to $t^*$ are weighted more, while units that are located further away are weighted less. This makes sense in the context of the identification assumption underlying the RD estimator, i.e. units close to the cutoff only differ in treatment assignment. Thus, treatment is considered quasi-random. 

Our crux is to translate this weighting scheme into a topological setting. Intuitively, we want to achieve the following: For each additional unit we observe after the treatment has been administered, evaluate whether this observation is \textit{topologically similar} to our pre-treatment mean. If it is, we want to assign a larger weight to it. If it is not, we want to assign less weight to it. Remember that we formalized the notion of topological similarity using the curvature $\kappa|\theta_i$, i.e. for unit $i$, the curvature at the projection angle encodes its topological neighborhood. Thus, if some unit is embedded close to what we would expect from our pre-treatment data, we want to assign a lot of weight to it since it is topologically similar. 

This thought process yields the following weighting approach. Let $\hat{\tau}(t^*)$ be the weighted treatment effect estimator, 

\[
  \hat{\tau}(t^*) = \sum_{j=n+1}^{m} \frac{w_j \cdot \theta_j^{t \geq t^*}}{m - n} - \sum_{i=1}^{n} \frac{w_i \cdot \theta_i^{t < t^*}}{n}
\]

For all pretreatment observations, $i \in \{1, \cdots, n\}$, the weight $w_i$ simply quantifies the difference of unit $i$'s projection angle $\theta_i$ from the mean pre-treatment projection angle $\bar{\theta}^{t < t^*}$. This is analogous to the canonical weighting scheme in literature. When we enter the post-treatment period, with observations $j \in \{n+1, \cdots, m\}$, the weight $w_j$ is drawn from a posterior with prior around the pre-treatment projection angle. Its likelihood is updated for each additional point in the post-treatment period. Thus, we formally define our weight function as follows: 

$$
w_i(\kappa|\theta) = 
  \frac{1}{\|\kappa_i^{t<t^*}|\textcolor{darkorange}{\{\theta_i\}_{i=1}^{t}} - \kappa^{t<t^*}|\bar{\theta}^{t<t^*}\|}
$$

Here, $\kappa^{t<t^*}|\theta_i$ denotes the Gaussian curvature on the pre-treatment manifold, evaluated at $\theta_i$. Meanwhile, $\kappa^{t<t^*}|\bar{\theta}^{t<t^*}$ denotes the Gaussian curvature on the pre-treatment manifold, evaluated at the mean pre-treatment projection angle. In the post-treatment period, this quantity is always non-random. 

Before we proceed, we want to emphasize the difference in interpretation of ${\{\theta_i\}_{i=1}^t}$ from the pre-treatment to the post-treatment case. In the pre-treatment case, ${\{\theta_i\}_{i=1}^t}$ represents the prior distribution using pre-treatment observations. In the post-treatment case, ${\{\theta_i\}_{i=1}^t}$ becomes a posterior distribution, where the prior distribution is updated with each post-treatment observation in $j \in \{n+1, \cdots, t\}$. This implies that we can express our data vector as a disjoint union of sets:

$$
\textcolor{darkorange}{\{\theta_i\}_{i=1}^t} = 
\begin{cases}
\{\theta_i\}_{i=1}^{t}, & \text{if } t < t^* \\ 
\{\theta_i\}_{i=1}^{n} \cup \{\theta_j\}_{j=n+1}^{t}, & \text{if } t \geq t^*
\end{cases}
$$

In the pre-treatment case, we have a canonical von Mises distribution. In the post-treatment case, we make use of our conjugacy result from Section~\ref{sc:directional-priors}. Naturally, in a causal setting, we will focus on the case $t \geq t^*$, i.e. when post-treatment data is available. This yields our hierarchical manifold RD estimator: 

\[
  \begin{aligned}
    \tau | &w_i \\
           &w_i = w_i(\kappa^{t<t^*}|\theta_i) \\
           &\theta_i \sim \mathcal{VM}(\mu, \sigma^2)
\end{aligned}
\]

where

\[
\begin{aligned}
  &\mu =  \frac{1/\kappa^{t<t^*}}{1/\kappa^{t<t^*} + 1/\kappa^{t \geq t^*}} \cdot \bar{\theta}^{t < t^*} + \frac{1/\kappa^{t \geq t^*}}{1/\kappa^{t < t^*} + 1/\kappa^{t \geq t^*}} \cdot \bar{\theta}^{t \geq t^*} \\
  &\sigma^2 = \sqrt{\left[\kappa^{t < t^*}\cos(\bar{\theta}^{t < t^*}) + \textcolor{darkorange}{\kappa^{t \geq t^*}\cos(\bar{\theta}^{t \geq t^*})}\right]^2 
  + 
\left[\kappa^{t < t^*}\sin(\bar{\theta}^{t < t^*}) + \textcolor{darkorange}{\kappa^{t \geq t^*}\sin(\bar{\theta}^{t \geq t^*})}\right]^2} 
\end{aligned}
\]

by the conjugacy results of Section~\ref{sc:conjugacy}. 

We want to emphasize that the Bayesian inference procedure is not the causal estimator $\hat{\tau}$ directly. Rather the weights as a function of $\kappa$ (evaluated at $\theta$). Posterior inference through the above mechanism is possible as soon as we observe at least one post-treatment unit. An immediate advantage of this estimation procedure is that we can update our estimates readily for each new observation. 

The manifold RD estimator is similar to weighted RD estimators in literature. A common and intuitive estimation procedure for RD estimators is the Local Linear Regression (LLR) framework, i.e. measure the LATE as the difference in post- and pre-treatment means, given that they are sufficiently close to the treatment cutoff $t^*$. \citet{Branson2019} considers a Bayesian RD setting where a Gaussian process prior is placed on the pre- and post-treatment mean response functions. We believe this work, although not directly related to the use of topological information in causal inference, encapsulates the desire for more flexible, data-driven, and principled causal inference through Bayesian paradigms.   


\subsection{Estimator Properties and Simulation}

An analytical derivation of our estimator's properties is simplified greatly by our conjugacy results. Naturally, outside of the von Mises parameterization, estimator properties can be determined by posterior sampling and numerical methods. To provide further insight into the estimator, we will analytically derive its first two moments and provide a brief simulation. 

The expectation of $\hat{\tau}(t^*)$, where we let $w_{i,j} = w_{i,j}(\kappa|\mu)$ denote the respective weights, is the following:
\[
  \begin{aligned}
    \mathbb{E}(\hat{\tau}(t^*)) &= \sum_{j=n+1}^{m} \frac{w_j \cdot \theta_j^{t \geq t^*}}{w_j} - \sum_{i=1}^{n} \frac{w_i \cdot \theta_i^{t < t^*}}{w_i}
  \end{aligned}
\]

Intuitively, we see that our manifold RD estimator is a topologically weighted average of post-treatment observations, that accounts for the pre-treatment "baseline". For our variance, 

$$
\begin{aligned}
\mathbb{V}(\hat{\tau}(t^*)) &= \mathbb{V}\left(\sum_{j=n+1}^{m} \frac{w_j \cdot \theta_j^{t \geq t^*}}{w_j} - \sum_{i=1}^{n} \frac{w_i \cdot \theta_i^{t < t^*}}{w_i}\right) \\
                            &= \mathbb{V}\left(\sum_{j=n+1}^{m} \frac{w_j \cdot \theta_j^{t \geq t^*}}{w_j}\right) + \mathbb{V}\left(\sum_{i=1}^{n} \frac{w_i \cdot \theta_i^{t < t^*}}{w_i}\right) \\
                            &\underbrace{- 2\operatorname{Cov}\left(\sum_{j=n+1}^{m} \frac{w_j \cdot \theta_j^{t \geq t^*}}{w_j}, \sum_{i=1}^{n} \frac{w_i \cdot \theta_i^{t < t^*}}{w_i}\right)}_{=0}
\end{aligned}
$$

Since the pre-treatment and post-treatment observations are assumed to be independent, the covariance term equals zero. The evaluation of the variance term depends on the underlying curvature of the manifold, and is thus highly dependent on the choice of manifold.

Returning to our spherical example, our moment expressions simplify greatly. This is because in a sphere, our curvature is constant everywhere, thus yielding no variation in the weights. Therefore, our manifold RD estimator reduces to our canonical RD estimator, i.e. the difference of post- and pre-treatment angles. 

$$
\begin{aligned}
  \mathbb{E}(\hat{\tau}(t^*)) &= \mu^{t \geq t^*} - \mu^{t < t^*} \\ 
  \mathbb{V}(\hat{\tau}(t^*)) &= \frac{(\kappa^{t \geq t^*})^{-1}}{n-m} + \frac{(\kappa^{t < t^*})^{-1}}{n} 
\end{aligned}
$$

Although the spherical example is thus somewhat trivial, we want to present a simulation study of the manifold RD estimator when both manifolds are spheres. We recycle the toy example from Section~\ref{sc:conjugacy}. Suppose we have a post-treatment mean projection angle, $\bar{\theta}^{t \geq t^*} = \pi$ and a pre-treatment mean projection angle, $\bar{\theta}^{t < t^*} = \pi/2$. We consider both of these manifolds to be circles, but the post-treatment manifold has radius $2$. Thus, the manifold we have fit grows from a unit circle (radius = $1$) to a post-treatment circle with radius $2$. This is relevant to biomedical settings where we believe our object of interest, e.g., a tumor growth, grows equally in size after treatment. This means that while the pre-treatment Gaussian curvature stays 1, i.e. $\kappa^{t < t^*} = 1$, our post-treatment curvature is now $\kappa^{t \geq t^*} = \frac{1}{R^2} = \frac{1}{4}$. We simulate 500 draws and observe the following results: 


\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.95\textwidth]{../fig/rd_sim.png}
  \end{center}
  \caption{Distribution of Pre- and Post-Treatment Data, Treatment Effect, and Estimated Treatment Effect}\label{fig:rd-sim}
\end{figure}

We can see that our estimator captures the treatment effect well. We want to reiterate that in the spherical setting, the manifold RD estimator simplifies to the canonical estimator. That being said, it is reassuring to see well-behaved simulation-based results in this simplest of settings. 

\section{Discussion and Future Direction}

When compared to well-established LLR methods, the mean projection angles are simply a spherical reparameterization of the outcome variable of interest. This formulation naturally preserves identification assumptions required for causal interpretation while accommodating the topological abstractions within the principal manifold framework. The magnitude of the posterior variance directly reflects how drastically treatment has altered the local geometry of our outcome space, e.g., if we have a large difference, we want to encode that topological uncertainty into our LATE. 

An important point of emphasis is what this procedure is not. Namely, we are not simply updating our fitted manifold with the new data, and letting the posterior angle update iteratively with each new post-treatment observation. This is certainly an interesting angle in the context of statistical learning procedures (e.g., prediction tasks), but would complicate causal identification. Rather, we are using the pre-treatment manifold to borrow topological information for our post-treatment mean vector. In that sense, after having fit $\mathcal{M}_d^{t \geq t^*}$ to the post-treatment data only, we assign the prior mean (i.e. $\bar{\theta}^{t<t^*}$) a weight of zero, only using the post-treatment observations.

We round out this section with an appealing information-theoretic point of view. This manifold RD estimate is fundamentally analogous to the existing methods that penalize observations for being further away from treatment onset in time. The main difference is that this procedure accounts for the topological information of our embedding space. Existing methods are quick to discard this information and assume it away. Through our Bayesian topological weights, we now have a procedure for iteratively updating our beliefs in the underlying topological structure of our data. Each additional observation in the post-treatment period allows us to strengthen our beliefs in local topological changes. 

Future work could address the use of priors for the construction of the mean projection vectors in the pre- and post-treatment periods, respectively. That is, within the existing manifold RD framework, posit hyperpriors within each treatment period. This connects neatly to the idea of encoding global manifold prior information. A promising avenue within the broader topological data analysis is mmanifold learning drawing from persistent homology. To the author's knowledge, there currently exists no method to obtain causal estimates in a manifold RD setup. With that in mind, this idea is rudimentary and can benefit from more rigorous analysis and robust theoretical guarantees. 

\section{Code and Data Availability Statement}

Code necessary to reproduce the simulation study in this paper is available at the GitHub repository: \href{https://github.com/posmikdc/directional-priors}{https://github.com/posmikdc/directional-priors}

\newpage

%\appendix
%\section*{Appendix A.} \label{sc:app_a}
%\newpage 
\bibliography{references}

\end{document}
